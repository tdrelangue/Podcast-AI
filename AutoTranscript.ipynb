{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Auto-Transcription Project**\n",
    "\n",
    "### **Description**\n",
    "This notebook demonstrates the process of converting audio files into text using modern automatic speech recognition (ASR) techniques. The workflow involves loading audio files, preprocessing them, applying a transcription model, and exporting the results in a user-friendly format.\n",
    "\n",
    "---\n",
    "\n",
    "## **Notebook Structure**\n",
    "1. **Introduction**  \n",
    "   Overview of the notebook’s purpose and objectives.\n",
    "\n",
    "2. **Setup**  \n",
    "   Required libraries and configurations for the transcription process.\n",
    "\n",
    "3. **Data Preparation**  \n",
    "   Loading and exploring audio files to ensure proper format and quality.\n",
    "\n",
    "4. **Transcription**  \n",
    "   Applying a pretrained speech-to-text model to convert audio into text.\n",
    "\n",
    "5. **Results**  \n",
    "   Presenting the transcribed text and saving outputs for further analysis.\n",
    "\n",
    "6. **Conclusion and Future Work**  \n",
    "   Summary of findings, challenges, and potential improvements.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Features**\n",
    "- **Preprocessing:** Ensures audio files meet the requirements for ASR models.  \n",
    "- **Pretrained Model Usage:** Utilises state-of-the-art models for accurate transcription.  \n",
    "- **Scalability:** Processes multiple audio files in batch mode.  \n",
    "- **Output:** Saves transcriptions as text files for easy integration with other tools.\n",
    "\n",
    "---\n",
    "\n",
    "### **Usage Instructions**\n",
    "- Place all input audio files in a dedicated folder. Ensure they are in the correct format (e.g., `.wav`, 16kHz).  \n",
    "- Update configuration settings (e.g., file paths, model parameters) as needed.  \n",
    "- Run the notebook sequentially to complete the transcription process.  \n",
    "- Review and validate the transcribed text for quality assurance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n",
    "\n",
    "Here’s how to set up for your voice cloning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.36.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (9.5.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (0.5.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio[ffmpeg]) (6.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: alive-progress in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: about-time==4.2.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alive-progress) (4.2.1)\n",
      "Requirement already satisfied: grapheme==0.6.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from alive-progress) (0.6.0)\n",
      "ffmpeg version 2024-11-21-git-f298507323-essentials_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\n",
      "built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\n",
      "configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\n",
      "libavutil      59. 47.100 / 59. 47.100\n",
      "libavcodec     61. 25.102 / 61. 25.102\n",
      "libavformat    61.  9.100 / 61.  9.100\n",
      "libavdevice    61.  4.100 / 61.  4.100\n",
      "libavfilter    10.  6.101 / 10.  6.101\n",
      "libswscale      8.  9.101 /  8.  9.101\n",
      "libswresample   5.  4.100 /  5.  4.100\n",
      "libpostproc    58.  4.100 / 58.  4.100\n"
     ]
    }
   ],
   "source": [
    "%pip install imageio[ffmpeg]\n",
    "%pip install alive-progress\n",
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\tdrelangue\\appdata\\local\\temp\\pip-req-build-zl77n_my\n",
      "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numba in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper==20240930) (0.59.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper==20240930) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper==20240930) (2.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper==20240930) (4.66.4)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper==20240930) (10.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai-whisper==20240930) (0.4.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from numba->openai-whisper==20240930) (0.42.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->openai-whisper==20240930) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->openai-whisper==20240930) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->openai-whisper==20240930) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->openai-whisper==20240930) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->openai-whisper==20240930) (2021.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->openai-whisper==20240930) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\tdrelangue\\AppData\\Local\\Temp\\pip-req-build-zl77n_my'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.18.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/whisper.git\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Adjust the PyTorch URL based on your CUDA version; use `cu118` for CUDA 11.8 or `cpu` for non-GPU systems.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import csv\n",
    "from alive_progress import alive_bar\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tacotron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: torch>=2.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (4.66.4)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch-lightning) (0.11.9)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (69.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (2021.4.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.1.0->pytorch-lightning) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.1.0->pytorch-lightning) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchaudio in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchaudio) (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchaudio) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchaudio) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchaudio) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.3.1->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.3.1->torchaudio) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torchvision in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.18.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.3.1->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pyopencl in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2024.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyopencl) (1.26.4)\n",
      "Requirement already satisfied: platformdirs>=2.2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyopencl) (4.2.2)\n",
      "Requirement already satisfied: pytools>=2024.1.5 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyopencl) (2024.1.19)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytools>=2024.1.5->pyopencl) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install pytorch-lightning\n",
    "%pip install torchaudio\n",
    "%pip install torchvision \n",
    "%pip install pandas\n",
    "%pip install pyopencl\n",
    "%pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tdrelangue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\tdrelangue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytools\\persistent_dict.py:52: RecommendedHashNotFoundWarning: Unable to import recommended hash 'siphash24.siphash13', falling back to 'hashlib.sha256'. Run 'python3 -m pip install siphash24' to install the recommended hash.\n",
      "  warn(\"Unable to import recommended hash 'siphash24.siphash13', \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchaudio.models import Tacotron2\n",
    "import torch.optim as optim\n",
    "import pyopencl as clpr\n",
    "import icecream as ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Transcribe Audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += r\";C:\\Users\\tdrelangue\\ffmpeg\\bin\"\n",
    "model = whisper.load_model(\"base\")  # Choose a model size\n",
    "audio_folder = \"Audios\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(audio_folder=audio_folder, dataprep=False):\n",
    "    metadata=[]\n",
    "    with alive_bar(len(os.listdir(audio_folder)),force_tty=True) as bar:\n",
    "        for filename in os.listdir(audio_folder):\n",
    "            if filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n",
    "                audio_path = os.path.join(os.getcwd(),audio_folder, filename)\n",
    "\n",
    "                # Load model and transcribe\n",
    "                result = model.transcribe(audio_path, fp16=False)\n",
    "                if not dataprep:\n",
    "                    text_path = audio_path.replace(\".wav\", \".txt\").replace(\".mp3\", \".txt\").replace(\"Audio\", \"transcript\")\n",
    "                    with open(text_path, \"w\") as f:\n",
    "                        f.write(result[\"text\"])\n",
    "                else :\n",
    "                    metadata.append([audio_path, result[\"text\"]])\n",
    "            bar()\n",
    "        \n",
    "    \n",
    "    if dataprep:\n",
    "        with open(\"dataset\\metadata.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=\"|\")\n",
    "            writer.writerows(metadata)\n",
    "        print(\"Data transcribed !\")\n",
    "\n",
    "#transcribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 4/4 [100%] in 6:05.6 (0.01/s)        \n"
     ]
    }
   ],
   "source": [
    "# Load audio\n",
    "with alive_bar(len(os.listdir(audio_folder)),force_tty=True) as bar:\n",
    "    for file, filename in enumerate(os.listdir(audio_folder)):\n",
    "        audio = None\n",
    "        audio_path = os.path.join(os.getcwd(),audio_folder, filename)\n",
    "\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            audio = AudioSegment.from_file(f\"{audio_path}\", format=\"mp3\")\n",
    "        elif filename.endswith(\".wav\"):\n",
    "            audio = AudioSegment.from_file(f\"{audio_path}\", format=\"wav\")   \n",
    "\n",
    "        if audio:\n",
    "            # Split on silence\n",
    "            chunks = split_on_silence(audio, min_silence_len=200, silence_thresh=-40)\n",
    "\n",
    "            # Export clips\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                os.makedirs(\"dataset/wavs\", exist_ok=True)\n",
    "                chunk_name = f\"dataset/wavs/clip_{file}_{i:04d}.wav\"\n",
    "                chunk.export(chunk_name, format=\"wav\")\n",
    "                y, sr = librosa.load(chunk_name, sr=16000, mono=True)  # Load and resample\n",
    "                sf.write(chunk_name, y, sr)  # Save the processed file\n",
    "        bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████| 1153/1153 [100%] in 1:29:51.7 (0.21/s\n",
      "Data transcribed !\n"
     ]
    }
   ],
   "source": [
    "transcribe(audio_folder=f\"dataset/wavs\",dataprep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Training and Validation sets\n",
    "useless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"dataset/metadata.csv\"  # Path to your metadata.csv\n",
    "filelists_folder = \"filelists\"          # Output folder for filelists\n",
    "os.makedirs(filelists_folder, exist_ok=True)  # Ensure the filelists folder exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training filelist created: filelists\\train_filelist.txt\n",
      "Validation filelist created: filelists\\val_filelist.txt\n"
     ]
    }
   ],
   "source": [
    "# Read metadata\n",
    "with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Shuffle lines\n",
    "random.shuffle(lines)\n",
    "\n",
    "# Split into training and validation sets\n",
    "split_ratio = 0.85  # 90% train, 10% validation\n",
    "split_index = int(len(lines) * split_ratio)\n",
    "train_lines = lines[:split_index]\n",
    "val_lines = lines[split_index:]\n",
    "\n",
    "# Write train_filelist.txt\n",
    "train_file_path = os.path.join(filelists_folder, \"train_filelist.txt\")\n",
    "with open(train_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_lines)\n",
    "\n",
    "# Write val_filelist.txt\n",
    "val_file_path = os.path.join(filelists_folder, \"val_filelist.txt\")\n",
    "with open(val_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(val_lines)\n",
    "\n",
    "print(f\"Training filelist created: {train_file_path}\")\n",
    "print(f\"Validation filelist created: {val_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tacotron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer:\n",
    "    def __init__(self):\n",
    "        # Define a character set. You can expand this if needed.\n",
    "        self.char_to_id = {char: idx for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \")}  # Include space\n",
    "        self.id_to_char = {idx: char for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \")}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Check if the input is a string; if not, pass it as is\n",
    "        if isinstance(text, torch.Tensor):\n",
    "            return text  # Already encoded, return as is\n",
    "        if not isinstance(text, str):\n",
    "            text = self.decode(ids=text)\n",
    "\n",
    "        # Encode text to a list of indices and convert to a tensor\n",
    "        token_indices = [self.char_to_id[char] for char in text.lower() if char in self.char_to_id]\n",
    "        return torch.tensor(token_indices, dtype=torch.long)\n",
    "\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # Decode a tensor or list of indices back to text\n",
    "        if isinstance(ids, torch.Tensor):  # If input is a tensor, convert to a list\n",
    "            ids = ids.tolist()\n",
    "        if isinstance(ids, float):  # If input is a tensor, convert to a list\n",
    "            return f'{ids}'\n",
    "        return ''.join([self.id_to_char[idx] for idx in ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sacremoses in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (2024.5.15)\n",
      "Requirement already satisfied: click in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sacremoses) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tdrelangue\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install sacremoses\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "# chargement du tokenizer associé au modèle bert-base-multilingual-cased\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# les classes AutoTokenizer / AutoModel / AutoConfig permettent de \"deviner\" la classe exacte\n",
    "# des différents modèles, d'après leur nom\n",
    "# (le nom \"bert-base-multilingual-cased\" est associé à un BertModel, BertTokenizer et BertConfig)\n",
    "print(type(tokenizer_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, metadata_path, audio_dir, cfg, target_length=None):\n",
    "        super().__init__()\n",
    "        self.metadata = pd.read_csv(metadata_path, sep=\"|\", header=None, names=[\"file\", \"text\"])\n",
    "        self.audio_dir = audio_dir\n",
    "        self.target_length = target_length  # Specify target length for padding/truncation\n",
    "\n",
    "        # Create MelSpectrogram transform\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=22050,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=cfg[\"model\"][\"mel_channels\"],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get file path and text label\n",
    "        row = self.metadata.iloc[idx]\n",
    "        audio_path = os.path.join(self.audio_dir, row[\"file\"])\n",
    "        label_text = row[\"text\"]\n",
    "\n",
    "        # Load audio and convert to mel spectrogram\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        if sample_rate != 22050:\n",
    "            resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=22050)\n",
    "            waveform = resample(waveform)\n",
    "        mel_spectrogram = self.mel_transform(waveform).squeeze(0)  # Remove channel dim if mono\n",
    "\n",
    "        return mel_spectrogram, label_text\n",
    "\n",
    "    def pad_or_truncate(self, mel_spectrogram, target_length):\n",
    "        # Pad or truncate the mel spectrogram to the target length\n",
    "        if mel_spectrogram.size(1) < target_length:\n",
    "            # Pad with zeros (along the time dimension)\n",
    "            padding = target_length - mel_spectrogram.size(1)\n",
    "            mel_spectrogram = torch.nn.functional.pad(mel_spectrogram, (0, padding), mode='constant', value=0)\n",
    "        else:\n",
    "            # Truncate if it's too long\n",
    "            mel_spectrogram = mel_spectrogram[:, :target_length]\n",
    "\n",
    "        return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Separate texts and mel spectrograms from the batch\n",
    "    texts, mel_spectrograms = zip(*batch)\n",
    "\n",
    "    # Debugging: Print the shape of the first mel spectrogram\n",
    "    print(f\"First mel spectrogram shape (before padding): {mel_spectrograms[0].shape}\")\n",
    "\n",
    "    # Pad texts (assumes each text is already a 1D tensor)\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Ensure mel spectrograms are padded along the time dimension\n",
    "    max_len = max(mel.shape[1] for mel in mel_spectrograms)  # Max time dimension\n",
    "    padded_mels = []\n",
    "    for mel in mel_spectrograms:\n",
    "        pad_len = max_len - mel.shape[1]\n",
    "        padded_mel = torch.nn.functional.pad(\n",
    "            mel,\n",
    "            (0, pad_len),  # Padding along the time dimension\n",
    "            mode=\"constant\",\n",
    "            value=0.0,  # Use 0.0 for padding\n",
    "        )\n",
    "        padded_mels.append(padded_mel)\n",
    "\n",
    "    # Stack into a batch tensor\n",
    "    mel_batch = torch.stack(padded_mels, dim=0)  # (batch_size, num_mels, max_len)\n",
    "\n",
    "    return padded_texts, mel_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2TTS(LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = Tacotron2(cfg[\"model\"])\n",
    "        self.mel_channels = cfg[\"model\"][\"mel_channels\"]  # Corresponds to mel_channels\n",
    "        self.hidden_channels = cfg[\"model\"][\"hidden_channels\"]\n",
    "        self.attention_dim = cfg[\"model\"][\"attention_dim\"]\n",
    "        self.default_mel_length = 80\n",
    "        self.cfg = cfg\n",
    "        # Add the embedding layer\n",
    "        self.embedding = nn.Embedding(cfg[\"model\"][\"vocab_size\"], cfg[\"model\"][\"embedding_dim\"])  # Define the embedding layer\n",
    "    \n",
    "    def forward(self, text, mel_spectrogram, token_lengths=None ,mel_specgram_lengths=None):\n",
    "        # Ensure `mel_spectrogram` is a tensor, or create a placeholder if `None`\n",
    "        if mel_spectrogram is None:\n",
    "            mel_spectrogram = torch.zeros((text.size(0), self.mel_channels, self.default_mel_length), \n",
    "                                        dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        mel_spectrogram = torch.tensor(mel_spectrogram, dtype=torch.float32) if not isinstance(mel_spectrogram, torch.Tensor) else mel_spectrogram\n",
    "        \n",
    "        # Ensure token_lengths is a tensor\n",
    "        token_lengths = torch.tensor(token_lengths, dtype=torch.long) if not isinstance(token_lengths, torch.Tensor) else token_lengths\n",
    "        mel_specgram_lengths = torch.tensor(mel_spectrogram, dtype=torch.long) if not isinstance(mel_specgram_lengths, torch.Tensor) else mel_specgram_lengths\n",
    "        # Sort token_lengths in descending order and get the sorted indices\n",
    "        sorted_lengths, sorted_idx = torch.sort(token_lengths, descending=True)\n",
    "        sorted_text = text[sorted_idx]  # Sort text accordingly\n",
    "        \n",
    "        # Pack the padded sequences (tokens) with sorted lengths\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(sorted_text, sorted_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Forward pass through the model (you will use `packed_input` now)\n",
    "        embedded_inputs = self.embedding(packed_input.data).transpose(1, 2)  # Example of how to handle packed input\n",
    "        \n",
    "        encoder_outputs = self.encoder(embedded_inputs, sorted_lengths)\n",
    "        mel_specgram, gate_outputs, alignments = self.decoder(encoder_outputs, mel_spectrogram, memory_lengths=sorted_lengths)\n",
    "        \n",
    "        mel_specgram_postnet = self.postnet(mel_specgram)\n",
    "        \n",
    "        return mel_specgram_postnet\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Unpack the batch\n",
    "        text, mel_spectrogram = batch\n",
    "\n",
    "        # Ensure token lengths and spectrogram lengths are tensors\n",
    "        token_lengths = torch.tensor([text.shape[1]] * text.shape[0], dtype=torch.long, device=text.device)\n",
    "        mel_specgram_lengths = torch.tensor([mel_spectrogram.shape[2]] * mel_spectrogram.shape[0], dtype=torch.long, device=mel_spectrogram.device)\n",
    "\n",
    "        # Forward pass\n",
    "        mel_spectrogram_pred = self.forward(text=text, mel_spectrogram=mel_spectrogram, token_lengths=token_lengths, mel_specgram_lengths=mel_specgram_lengths)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(mel_spectrogram_pred, mel_spectrogram)\n",
    "        \n",
    "        # Log the loss for monitoring\n",
    "        # Print debug information \n",
    "        # print(f\"text type: {type(text)}, text shape: {text.shape}\") \n",
    "        # print(f\"mel_spectrogram type: {type(mel_spectrogram)}, mel_spectrogram shape: {mel_spectrogram.shape}\")\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, mel_spectrogram = batch\n",
    "        token_lengths = torch.tensor([text.shape[1]] * text.shape[0], dtype=torch.long, device=text.device)  # Example: length per sample\n",
    "        mel_spectrogram_pred = self.forward(text=text, mel_spectrogram=mel_spectrogram, token_lengths=token_lengths)\n",
    "        loss = torch.nn.functional.mse_loss(mel_spectrogram_pred, mel_spectrogram)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        text, mel_spectrogram = batch\n",
    "        token_lengths = torch.tensor([text.shape[1]] * text.shape[0], dtype=torch.long, device=text.device)  # Example: length per sample\n",
    "        mel_spectrogram_pred = self.forward(text=text, mel_spectrogram=mel_spectrogram, token_lengths=token_lengths)\n",
    "        loss = torch.nn.functional.mse_loss(mel_spectrogram_pred, mel_spectrogram)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.cfg[\"trainer\"][\"lr\"])\n",
    "\n",
    "    def print_batch_properties(self, batch):\n",
    "        \"\"\"\n",
    "        Prints the properties of the batch components.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of data, expected to be a tuple or a list.\n",
    "        \"\"\"\n",
    "        if not isinstance(batch, (tuple, list)):\n",
    "            print(\"Batch is not a tuple or list. Type:\", type(batch))\n",
    "            return\n",
    "\n",
    "        print(\"Batch contains\", len(batch), \"elements.\")\n",
    "        for i, item in enumerate(batch):\n",
    "            print(f\"--- Element {i} ---\")\n",
    "            print(f\"Type: {type(item)}\")\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                print(f\"Shape: {item.shape}\")\n",
    "                print(f\"Dtype: {item.dtype}\")\n",
    "            elif hasattr(item, \"__len__\"):\n",
    "                print(f\"Length: {len(item)}\")\n",
    "            else:\n",
    "                print(\"No additional properties available.\")\n",
    "            print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "metadata_path = \"dataset/metadata.csv\"\n",
    "audio_dir = \"dataset/wavs\"\n",
    "metadata = pd.read_csv(metadata_path, sep=\"|\", header=None, names=[\"file\", \"text\"])\n",
    "\n",
    "# Combine all texts from the dataset\n",
    "all_texts = metadata[\"text\"].dropna().tolist()\n",
    "\n",
    "tokenized_texts=[]\n",
    "# Tokenize the texts using the tokenizer\n",
    "for text in all_texts:\n",
    "    \n",
    "    try:\n",
    "        tokenized_text = tokenizer_bert(text, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    except:\n",
    "        print(text)\n",
    "        print(type(text))\n",
    "        raise ValueError(text)\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "\n",
    "# Dataset setup\n",
    "vocab_size = len(tokenizer_bert.get_vocab())\n",
    "\n",
    "\n",
    "# Configuration\n",
    "cfg = {\n",
    "    \"model\": {\n",
    "        \"mel_channels\": 80,\n",
    "        \"hidden_channels\": 128,\n",
    "        \"attention_dim\": 128,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embedding_dim\":512\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 10,\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 16,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming MelSpectrogramDataset is defined\n",
    "dataset = MelSpectrogramDataset(metadata_path, audio_dir, cfg)\n",
    "\n",
    "# Split dataset into training and validation\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(0.2 * dataset_size)  # 20% validation\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg[\"trainer\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=8,  # Optional: Adjust depending on your CPU cores for faster data loading\n",
    "    pin_memory=True,\n",
    "    collate_fn=lambda batch: (\n",
    "        torch.stack([item[0] for item in batch]),  # Stack mel spectrograms\n",
    "        torch.stack([item[1] for item in batch]),  # Stack labels (tokenized or numeric)\n",
    "    )\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg[\"trainer\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=4,  # Optional: Adjust depending on your CPU cores for faster data loading\n",
    "    pin_memory=True,  # Optional: To speed up data transfer to GPU if using one\n",
    "    collate_fn=lambda batch: (\n",
    "        torch.stack([item[0] for item in batch]),  # Stack mel spectrograms\n",
    "        torch.stack([item[1] for item in batch]),  # Stack labels (tokenized or numeric)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<pyopencl.Platform 'Intel(R) OpenCL Graphics' at 0x257eed86840>]\n",
      "Available devices: [<pyopencl.Device 'Intel(R) UHD Graphics' on 'Intel(R) OpenCL Graphics' at 0x257ef462cf0>]\n",
      "Using device: Intel(R) UHD Graphics\n"
     ]
    }
   ],
   "source": [
    "print(clpr.get_platforms())\n",
    "platforms = clpr.get_platforms()\n",
    "devices = platforms[0].get_devices()\n",
    "print(f\"Available devices: {devices}\")\n",
    "context = clpr.Context(devices=[devices[0]])  # Select the first device\n",
    "print(f\"Using device: {devices[0].name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Tacotron2TTS(cfg)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_num_threads(8) \n",
    "model = model.to(device)\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# Define loss function (e.g., CrossEntropyLoss for classification)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg[\"trainer\"][\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                        | ▆█▆ 0/4060 [0%] in 6:46:06 (~0s, 0.0/ █▆▄ 0/4060 [0%] in 20:50 (~0s, 0.0/s) ▁▃▅ 0/4060 [0%] in 23:40 (~0s, 0.0/s) ▇▇▅ 0/4060 [0%] in 27:08 (~0s, 0.0/s) ▂▂▄ 0/4060 [0%] in 28:37 (~0s, 0.0/s) ▅▃▁ 0/4060 [0%] in 29:03 (~0s, 0.0/s) ▃▅▇ 0/4060 [0%] in 45:19 (~0s, 0.0/s) ▃▅▇ 0/4060 [0%] in 47:46 (~0s, 0.0/s) ▆█▆ 0/4060 [0%] in 3:08:56 (~0s, 0.0/ ▂▄▆ 0/4060 [0%] in 6:26:55 (~0s, 0.0/"
     ]
    }
   ],
   "source": [
    "with alive_bar(cfg[\"trainer\"][\"max_epochs\"]*len(train_loader)*7,force_tty=True) as bar:\n",
    "    for epoch in range(cfg[\"trainer\"][\"max_epochs\"]):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            mel_spectrograms, labels = batch  # Unpack batch into spectrograms and labels\n",
    "            bar()\n",
    "            \n",
    "            # Move to GPU if available\n",
    "            mel_spectrograms = mel_spectrograms.to(device)\n",
    "            labels = labels.to(device)  # Get labels from your dataset (you may need to modify this based on your use case)\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            bar()\n",
    "            # Forward pass\n",
    "            outputs = model(mel_spectrograms)\n",
    "            bar()\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            bar()\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            bar()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            bar()\n",
    "            running_loss += loss.item()\n",
    "            bar()\n",
    "\n",
    "        # Print epoch loss\n",
    "        print(f\"Epoch {epoch+1}/{cfg['trainer']['max_epochs']}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tacotron2 import synthesize\n",
    "\n",
    "audio = synthesize(\n",
    "    checkpoint=\"checkpoints/latest_model.pth\",\n",
    "    text=\"Hello, this is a test synthesis.\"\n",
    ")\n",
    "with open(\"output.wav\", \"wb\") as f:\n",
    "    f.write(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
